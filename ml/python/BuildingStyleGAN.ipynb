{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict as odict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nnf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Add:\n",
    "\n",
    "Sequences of stuff (list comprehensions)\n",
    "\n",
    "Tensor intro\n",
    "\n",
    "Drawing with tensors\n",
    "\n",
    "Math = Drawing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequences\n",
    "\n",
    "## Expansion and Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors are Drawings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math is Drawings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Settings:\n",
    "    LatentDimension = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping Network\n",
    "\n",
    "The mapping network is stated to be a nonlinear function:\n",
    "\n",
    "$$f : Z \\rightarrow W$$\n",
    "\n",
    "The authors state that this function is implemented practically as a multilayer perceptron (MLP) with 8 layers and that both spaces $Z$ and $W$ are set to be 512-dimensional.\n",
    "\n",
    "We could state this more explicitly as:\n",
    "\n",
    "$$ Z, W \\in \\mathbb{R}^{512} $$\n",
    "\n",
    "All that this means is that both $Z$ and $W$ are vectors of real numbers that have 512 entries ( `[1.1, 2.65, 3.141, ..., 6.022]` ).\n",
    "\n",
    "### Multilayer Perceptron\n",
    "\n",
    "But what, exactly, is a \"multilayer perceptron\"?\n",
    "\n",
    "An MLP is a very simple kind of neural network that simply takes a vector input, multiplies it with a weight matrix to get another vector, and then repeats for some number of layers. Formally:\n",
    "\n",
    "$$ x \\in \\mathbb{R}^{1 \\times m} $$\n",
    "$$ w \\in \\mathbb{R}^{m \\times n} $$\n",
    "$$ y \\in \\mathbb{R}^{1 \\times n} $$\n",
    "\n",
    "This is essentially just a vector, matrix product. If $m \\gt n$ then the layer will be performing data reduction, if $m \\lt n$ then it will be performing data expansion. Notably, if the weight matrix $w$ is square, $x$ and $y$ will be the same dimension, and this is what is happening in the Mapping Network. There is also no mention of a nonlinearity applied to the Mapping Network in the paper, so our construction in code is very straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MappingNetwork(nn.Sequential):\n",
    "    def __init__(self, layer_count=8, latent_dim=512):\n",
    "        super(MappingNetwork, self).__init__()\n",
    "\n",
    "        for layer_number in range(layer_count):\n",
    "            layer_name = \"linear_{}\".format(layer_number)\n",
    "            layer = nn.Linear(latent_dim, latent_dim)\n",
    "            self.add_module(layer_name, layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math Note\n",
    "\n",
    "$$ f \\sim mn $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthesis Network\n",
    "\n",
    "The authors' diagram of the Synthesis Network shows a repeating block of upsample, convolution, noise scaling/addition, and a function that they define called `AdaIN`.\n",
    "$$ W \\in \\mathbb{R}^n $$\n",
    "$$ Y \\in \\mathbb{R}^{2n} $$\n",
    "$$ A : W \\rightarrow Y $$\n",
    "\n",
    "$Y$ can be thought of as a style space where the scalar components are parameters that control both how strongly feature maps in $x$ are carried forward, and how much it is shifted around the style space.\n",
    "\n",
    "$$ AdaIN(x_i, y) = y_{s, i}\\frac{x_i - \\mu(x_i)}{\\sigma(x_i)} + y_{b, i} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A(nn.Module):\n",
    "    def __init__(self, in_features, w_dim=512):\n",
    "        super(A, self).__init__()\n",
    "        self.affine = nn.Linear(w_dim, 2 * in_features)\n",
    "    \n",
    "    def forward(self, w):\n",
    "        return self.affine(w).reshape(2, -1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class B(nn.Module):\n",
    "    def __init__(self, height, width, num_features):\n",
    "        super(B, self).__init__()\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.num_features = num_features\n",
    "        \n",
    "        self.noise_image = torch.randn(1, 1, height, width)\n",
    "        \n",
    "        self.scaling_factors = torch.nn.Parameter(data=torch.randn(1, num_features, 1, 1), requires_grad=True)\n",
    "        \n",
    "    def forward(self):\n",
    "        return self.scaling_factors.expand(1, -1, self.height, self.width) * self.noise_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaIN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AdaIN, self).__init__()\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        mu_x    = x.mean(dim=(0, 2, 3)).reshape(1, -1, 1, 1)\n",
    "        sigma_x = x.std(dim=(0, 2, 3)).reshape(1, -1, 1, 1)\n",
    "        \n",
    "        normed_x = (x - mu_x) / sigma_x\n",
    "        \n",
    "        y = y.reshape(2, -1, 1, 1)\n",
    "        \n",
    "        return (y[0, :] * x) + y[1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PixelNorm\n",
    "From the [Progressive Growing of GANs paper](https://arxiv.org/pdf/1710.10196.pdf), section 4.2, the authors detail the per-pixel normalization function as:\n",
    "\n",
    "$$ b_{x, y} = \\frac{a_{x, y}}{\\sqrt{\\frac{1}{n}\\Sigma_{j=0}^{n-1}{(a^{j}_{x, y})^2 + \\epsilon}}} $$\n",
    "\n",
    "where $\\epsilon = 10^{-8}$ and $n$ is the number of feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PixelNorm, self).__init__()\n",
    "        self.epsilon = 10 ** -8\n",
    "        \n",
    "    def forward(self, x):\n",
    "        n, c, h, w = x.shape\n",
    "        d = x.pow(2)\n",
    "        d = d.sum(dim=(1)) + self.epsilon\n",
    "        d = d.mul(1 / c)\n",
    "        d = d.sqrt()\n",
    "        d = d.unsqueeze(1)\n",
    "\n",
    "        return x / d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, (3, 3), 1, 1)\n",
    "        self.conv.weight.data.normal_(0, 1)\n",
    "        self.conv.bias.data.fill_(0)\n",
    "        \n",
    "        self.norm = PixelNorm()\n",
    "        self.act = nn.LeakyReLU(negative_slope=0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        return x     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynthesisBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, height, width, w_dim=512):\n",
    "        super(SynthesisBlock, self).__init__()\n",
    "        \n",
    "        self.upsample = nn.UpsamplingBilinear2d((height, width))\n",
    "        self.conv0 =    ConvBlock(in_channels, out_channels)\n",
    "        self.b0 =       B(height, width, out_channels)\n",
    "        self.a0 =       A(out_channels, w_dim=w_dim)\n",
    "        self.adain0 =   AdaIN()\n",
    "        \n",
    "        self.conv1 =    ConvBlock(out_channels, out_channels)\n",
    "        self.b1 =       B(height, width, out_channels)\n",
    "        self.a1 =       A(out_channels, w_dim=w_dim)\n",
    "        self.adain1 =   AdaIN()\n",
    "    \n",
    "    def forward(self, tensor_dict):\n",
    "\n",
    "        x = tensor_dict[\"x\"]\n",
    "        w = tensor_dict[\"w\"]\n",
    "        \n",
    "        x = self.upsample(x)\n",
    "        x = self.conv0(x)\n",
    "        x = x + self.b0()\n",
    "        y = self.a0(w)\n",
    "        x = self.adain0(x, y)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = x + self.b1()\n",
    "        y = self.a1(w)\n",
    "        x = self.adain1(x, y)\n",
    "        \n",
    "        return {\"x\": x, \"w\": w}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, height, width, w_dim=512):\n",
    "        super(InputBlock, self).__init__()\n",
    "        \n",
    "        self.conv0 =    ConvBlock(in_channels, out_channels)\n",
    "        self.b0 =       B(height, width, out_channels)\n",
    "        self.a0 =       A(out_channels, w_dim=w_dim)\n",
    "        self.adain0 =   AdaIN()\n",
    "        \n",
    "        self.b1 =       B(height, width, out_channels)\n",
    "        self.a1 =       A(out_channels, w_dim=w_dim)\n",
    "        self.adain1 =   AdaIN()\n",
    "    \n",
    "    def forward(self, tensor_dict):\n",
    "\n",
    "        x = tensor_dict[\"x\"]\n",
    "        w = tensor_dict[\"w\"]\n",
    "        \n",
    "        x = self.conv0(x)\n",
    "        x = x + self.b0()\n",
    "        y = self.a0(w)\n",
    "        x = self.adain0(x, y)\n",
    "\n",
    "        x = x + self.b1()\n",
    "        y = self.a1(w)\n",
    "        x = self.adain1(x, y)\n",
    "        \n",
    "        return {\"x\": x, \"w\": w}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputBlock(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(OutputBlock, self).__init__()\n",
    "        \n",
    "        self.to_rgb = nn.Conv2d(input_channels, 3, (1, 1), 1, 1)\n",
    "    \n",
    "    def forward(self, tensor_dict):\n",
    "        x = tensor_dict[\"x\"]\n",
    "        w = tensor_dict[\"w\"]\n",
    "        \n",
    "        x = self.to_rgb(x)\n",
    "        \n",
    "        return {\"x\": x, \"w\": w}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleGAN(nn.Module):\n",
    "    def __init__(self, input_layer=None, layer_params=None, w_dim=512):\n",
    "        super(StyleGAN, self).__init__()\n",
    "        \n",
    "        if input_layer == None:\n",
    "            input_layer = InputBlock(512, 512, 4, 4)\n",
    "        \n",
    "        self.input = input_layer\n",
    "\n",
    "        self.main = nn.Sequential()\n",
    "\n",
    "        if layer_params == None:\n",
    "            layer_params = [\n",
    "                (512, 512,    8,    8, w_dim),\n",
    "                (512, 512,   16,   16, w_dim),\n",
    "                (512, 512,   32,   32, w_dim),\n",
    "                (512, 256,   64,   64, w_dim),\n",
    "                (256, 128,  128,  128, w_dim),\n",
    "                (128,  64,  256,  256, w_dim),\n",
    "                ( 64,  32,  512,  512, w_dim),\n",
    "                ( 32,  16, 1024, 1024, w_dim),\n",
    "            ]\n",
    "        \n",
    "        [self.main.add_module(\"sb_{}\".format(n), SynthesisBlock(*p)) for n, p in enumerate(layer_params)]\n",
    "    \n",
    "        final_out_channels = layer_params[-1][1]\n",
    "    \n",
    "        self.output = OutputBlock(final_out_channels)\n",
    "    \n",
    "    def forward(self, tensor_dict):\n",
    "        tensor_dict = self.input(tensor_dict)\n",
    "        tensor_dict = self.main(tensor_dict)\n",
    "\n",
    "        return self.output(tensor_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MappingNetwork(\n",
      "  (linear_0): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (linear_1): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (linear_2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (linear_3): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (linear_4): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (linear_5): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (linear_6): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (linear_7): Linear(in_features=512, out_features=512, bias=True)\n",
      ")\n",
      "StyleGAN(\n",
      "  (input): InputBlock(\n",
      "    (conv0): ConvBlock(\n",
      "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (norm): PixelNorm()\n",
      "      (act): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (b0): B()\n",
      "    (a0): A(\n",
      "      (affine): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    )\n",
      "    (adain0): AdaIN()\n",
      "    (b1): B()\n",
      "    (a1): A(\n",
      "      (affine): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    )\n",
      "    (adain1): AdaIN()\n",
      "  )\n",
      "  (main): Sequential(\n",
      "    (sb_0): SynthesisBlock(\n",
      "      (upsample): UpsamplingBilinear2d(size=(8, 8), mode=bilinear)\n",
      "      (conv0): ConvBlock(\n",
      "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm): PixelNorm()\n",
      "        (act): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "      (b0): B()\n",
      "      (a0): A(\n",
      "        (affine): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      )\n",
      "      (adain0): AdaIN()\n",
      "      (conv1): ConvBlock(\n",
      "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm): PixelNorm()\n",
      "        (act): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "      (b1): B()\n",
      "      (a1): A(\n",
      "        (affine): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      )\n",
      "      (adain1): AdaIN()\n",
      "    )\n",
      "    (sb_1): SynthesisBlock(\n",
      "      (upsample): UpsamplingBilinear2d(size=(16, 16), mode=bilinear)\n",
      "      (conv0): ConvBlock(\n",
      "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm): PixelNorm()\n",
      "        (act): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "      (b0): B()\n",
      "      (a0): A(\n",
      "        (affine): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      )\n",
      "      (adain0): AdaIN()\n",
      "      (conv1): ConvBlock(\n",
      "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm): PixelNorm()\n",
      "        (act): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "      (b1): B()\n",
      "      (a1): A(\n",
      "        (affine): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      )\n",
      "      (adain1): AdaIN()\n",
      "    )\n",
      "    (sb_2): SynthesisBlock(\n",
      "      (upsample): UpsamplingBilinear2d(size=(32, 32), mode=bilinear)\n",
      "      (conv0): ConvBlock(\n",
      "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm): PixelNorm()\n",
      "        (act): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "      (b0): B()\n",
      "      (a0): A(\n",
      "        (affine): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      )\n",
      "      (adain0): AdaIN()\n",
      "      (conv1): ConvBlock(\n",
      "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm): PixelNorm()\n",
      "        (act): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "      (b1): B()\n",
      "      (a1): A(\n",
      "        (affine): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      )\n",
      "      (adain1): AdaIN()\n",
      "    )\n",
      "    (sb_3): SynthesisBlock(\n",
      "      (upsample): UpsamplingBilinear2d(size=(64, 64), mode=bilinear)\n",
      "      (conv0): ConvBlock(\n",
      "        (conv): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm): PixelNorm()\n",
      "        (act): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "      (b0): B()\n",
      "      (a0): A(\n",
      "        (affine): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (adain0): AdaIN()\n",
      "      (conv1): ConvBlock(\n",
      "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm): PixelNorm()\n",
      "        (act): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "      (b1): B()\n",
      "      (a1): A(\n",
      "        (affine): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (adain1): AdaIN()\n",
      "    )\n",
      "    (sb_4): SynthesisBlock(\n",
      "      (upsample): UpsamplingBilinear2d(size=(128, 128), mode=bilinear)\n",
      "      (conv0): ConvBlock(\n",
      "        (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm): PixelNorm()\n",
      "        (act): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "      (b0): B()\n",
      "      (a0): A(\n",
      "        (affine): Linear(in_features=512, out_features=256, bias=True)\n",
      "      )\n",
      "      (adain0): AdaIN()\n",
      "      (conv1): ConvBlock(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm): PixelNorm()\n",
      "        (act): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "      (b1): B()\n",
      "      (a1): A(\n",
      "        (affine): Linear(in_features=512, out_features=256, bias=True)\n",
      "      )\n",
      "      (adain1): AdaIN()\n",
      "    )\n",
      "    (sb_5): SynthesisBlock(\n",
      "      (upsample): UpsamplingBilinear2d(size=(256, 256), mode=bilinear)\n",
      "      (conv0): ConvBlock(\n",
      "        (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm): PixelNorm()\n",
      "        (act): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "      (b0): B()\n",
      "      (a0): A(\n",
      "        (affine): Linear(in_features=512, out_features=128, bias=True)\n",
      "      )\n",
      "      (adain0): AdaIN()\n",
      "      (conv1): ConvBlock(\n",
      "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm): PixelNorm()\n",
      "        (act): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "      (b1): B()\n",
      "      (a1): A(\n",
      "        (affine): Linear(in_features=512, out_features=128, bias=True)\n",
      "      )\n",
      "      (adain1): AdaIN()\n",
      "    )\n",
      "    (sb_6): SynthesisBlock(\n",
      "      (upsample): UpsamplingBilinear2d(size=(512, 512), mode=bilinear)\n",
      "      (conv0): ConvBlock(\n",
      "        (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm): PixelNorm()\n",
      "        (act): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "      (b0): B()\n",
      "      (a0): A(\n",
      "        (affine): Linear(in_features=512, out_features=64, bias=True)\n",
      "      )\n",
      "      (adain0): AdaIN()\n",
      "      (conv1): ConvBlock(\n",
      "        (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm): PixelNorm()\n",
      "        (act): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "      (b1): B()\n",
      "      (a1): A(\n",
      "        (affine): Linear(in_features=512, out_features=64, bias=True)\n",
      "      )\n",
      "      (adain1): AdaIN()\n",
      "    )\n",
      "    (sb_7): SynthesisBlock(\n",
      "      (upsample): UpsamplingBilinear2d(size=(1024, 1024), mode=bilinear)\n",
      "      (conv0): ConvBlock(\n",
      "        (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm): PixelNorm()\n",
      "        (act): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "      (b0): B()\n",
      "      (a0): A(\n",
      "        (affine): Linear(in_features=512, out_features=32, bias=True)\n",
      "      )\n",
      "      (adain0): AdaIN()\n",
      "      (conv1): ConvBlock(\n",
      "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm): PixelNorm()\n",
      "        (act): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "      (b1): B()\n",
      "      (a1): A(\n",
      "        (affine): Linear(in_features=512, out_features=32, bias=True)\n",
      "      )\n",
      "      (adain1): AdaIN()\n",
      "    )\n",
      "  )\n",
      "  (output): OutputBlock(\n",
      "    (to_rgb): Conv2d(16, 3, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mn = MappingNetwork()\n",
    "sg = StyleGAN()\n",
    "\n",
    "print(mn)\n",
    "print(sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([[[[-0.0869, -0.0869, -0.0869,  ..., -0.0869, -0.0869, -0.0869],\n",
       "           [-0.0869, -0.0646, -0.0790,  ..., -0.0559, -0.0668, -0.0869],\n",
       "           [-0.0869, -0.0994, -0.0468,  ..., -0.0623, -0.0438, -0.0869],\n",
       "           ...,\n",
       "           [-0.0869, -0.0850, -0.0785,  ..., -0.0446, -0.0691, -0.0869],\n",
       "           [-0.0869, -0.0574, -0.0783,  ..., -0.0681, -0.0553, -0.0869],\n",
       "           [-0.0869, -0.0869, -0.0869,  ..., -0.0869, -0.0869, -0.0869]],\n",
       " \n",
       "          [[-0.1199, -0.1199, -0.1199,  ..., -0.1199, -0.1199, -0.1199],\n",
       "           [-0.1199, -0.1049, -0.1184,  ..., -0.1014, -0.1031, -0.1199],\n",
       "           [-0.1199, -0.1421, -0.1172,  ..., -0.1218, -0.0909, -0.1199],\n",
       "           ...,\n",
       "           [-0.1199, -0.1453, -0.1282,  ..., -0.1063, -0.1203, -0.1199],\n",
       "           [-0.1199, -0.1064, -0.1143,  ..., -0.1098, -0.1183, -0.1199],\n",
       "           [-0.1199, -0.1199, -0.1199,  ..., -0.1199, -0.1199, -0.1199]],\n",
       " \n",
       "          [[ 0.2024,  0.2024,  0.2024,  ...,  0.2024,  0.2024,  0.2024],\n",
       "           [ 0.2024,  0.1937,  0.1838,  ...,  0.1789,  0.1832,  0.2024],\n",
       "           [ 0.2024,  0.2107,  0.2141,  ...,  0.1963,  0.1731,  0.2024],\n",
       "           ...,\n",
       "           [ 0.2024,  0.2060,  0.2067,  ...,  0.2018,  0.1993,  0.2024],\n",
       "           [ 0.2024,  0.1948,  0.1988,  ...,  0.2076,  0.2167,  0.2024],\n",
       "           [ 0.2024,  0.2024,  0.2024,  ...,  0.2024,  0.2024,  0.2024]]],\n",
       " \n",
       " \n",
       "         [[[-0.0869, -0.0869, -0.0869,  ..., -0.0869, -0.0869, -0.0869],\n",
       "           [-0.0869, -0.0646, -0.0790,  ..., -0.0559, -0.0668, -0.0869],\n",
       "           [-0.0869, -0.0994, -0.0468,  ..., -0.0623, -0.0438, -0.0869],\n",
       "           ...,\n",
       "           [-0.0869, -0.0850, -0.0785,  ..., -0.0446, -0.0691, -0.0869],\n",
       "           [-0.0869, -0.0574, -0.0783,  ..., -0.0681, -0.0553, -0.0869],\n",
       "           [-0.0869, -0.0869, -0.0869,  ..., -0.0869, -0.0869, -0.0869]],\n",
       " \n",
       "          [[-0.1199, -0.1199, -0.1199,  ..., -0.1199, -0.1199, -0.1199],\n",
       "           [-0.1199, -0.1049, -0.1184,  ..., -0.1014, -0.1031, -0.1199],\n",
       "           [-0.1199, -0.1421, -0.1172,  ..., -0.1218, -0.0909, -0.1199],\n",
       "           ...,\n",
       "           [-0.1199, -0.1453, -0.1282,  ..., -0.1063, -0.1203, -0.1199],\n",
       "           [-0.1199, -0.1064, -0.1143,  ..., -0.1098, -0.1183, -0.1199],\n",
       "           [-0.1199, -0.1199, -0.1199,  ..., -0.1199, -0.1199, -0.1199]],\n",
       " \n",
       "          [[ 0.2024,  0.2024,  0.2024,  ...,  0.2024,  0.2024,  0.2024],\n",
       "           [ 0.2024,  0.1937,  0.1838,  ...,  0.1789,  0.1832,  0.2024],\n",
       "           [ 0.2024,  0.2107,  0.2141,  ...,  0.1963,  0.1731,  0.2024],\n",
       "           ...,\n",
       "           [ 0.2024,  0.2060,  0.2067,  ...,  0.2018,  0.1993,  0.2024],\n",
       "           [ 0.2024,  0.1948,  0.1988,  ...,  0.2076,  0.2167,  0.2024],\n",
       "           [ 0.2024,  0.2024,  0.2024,  ...,  0.2024,  0.2024,  0.2024]]],\n",
       " \n",
       " \n",
       "         [[[-0.0869, -0.0869, -0.0869,  ..., -0.0869, -0.0869, -0.0869],\n",
       "           [-0.0869, -0.0646, -0.0790,  ..., -0.0559, -0.0668, -0.0869],\n",
       "           [-0.0869, -0.0994, -0.0468,  ..., -0.0623, -0.0438, -0.0869],\n",
       "           ...,\n",
       "           [-0.0869, -0.0850, -0.0785,  ..., -0.0446, -0.0691, -0.0869],\n",
       "           [-0.0869, -0.0574, -0.0783,  ..., -0.0681, -0.0553, -0.0869],\n",
       "           [-0.0869, -0.0869, -0.0869,  ..., -0.0869, -0.0869, -0.0869]],\n",
       " \n",
       "          [[-0.1199, -0.1199, -0.1199,  ..., -0.1199, -0.1199, -0.1199],\n",
       "           [-0.1199, -0.1049, -0.1184,  ..., -0.1014, -0.1031, -0.1199],\n",
       "           [-0.1199, -0.1421, -0.1172,  ..., -0.1218, -0.0909, -0.1199],\n",
       "           ...,\n",
       "           [-0.1199, -0.1453, -0.1282,  ..., -0.1063, -0.1203, -0.1199],\n",
       "           [-0.1199, -0.1064, -0.1143,  ..., -0.1098, -0.1183, -0.1199],\n",
       "           [-0.1199, -0.1199, -0.1199,  ..., -0.1199, -0.1199, -0.1199]],\n",
       " \n",
       "          [[ 0.2024,  0.2024,  0.2024,  ...,  0.2024,  0.2024,  0.2024],\n",
       "           [ 0.2024,  0.1937,  0.1838,  ...,  0.1789,  0.1832,  0.2024],\n",
       "           [ 0.2024,  0.2107,  0.2141,  ...,  0.1963,  0.1731,  0.2024],\n",
       "           ...,\n",
       "           [ 0.2024,  0.2060,  0.2067,  ...,  0.2018,  0.1993,  0.2024],\n",
       "           [ 0.2024,  0.1948,  0.1988,  ...,  0.2076,  0.2167,  0.2024],\n",
       "           [ 0.2024,  0.2024,  0.2024,  ...,  0.2024,  0.2024,  0.2024]]],\n",
       " \n",
       " \n",
       "         [[[-0.0869, -0.0869, -0.0869,  ..., -0.0869, -0.0869, -0.0869],\n",
       "           [-0.0869, -0.0646, -0.0790,  ..., -0.0559, -0.0668, -0.0869],\n",
       "           [-0.0869, -0.0994, -0.0468,  ..., -0.0623, -0.0438, -0.0869],\n",
       "           ...,\n",
       "           [-0.0869, -0.0850, -0.0785,  ..., -0.0446, -0.0691, -0.0869],\n",
       "           [-0.0869, -0.0574, -0.0783,  ..., -0.0681, -0.0553, -0.0869],\n",
       "           [-0.0869, -0.0869, -0.0869,  ..., -0.0869, -0.0869, -0.0869]],\n",
       " \n",
       "          [[-0.1199, -0.1199, -0.1199,  ..., -0.1199, -0.1199, -0.1199],\n",
       "           [-0.1199, -0.1049, -0.1184,  ..., -0.1014, -0.1031, -0.1199],\n",
       "           [-0.1199, -0.1421, -0.1172,  ..., -0.1218, -0.0909, -0.1199],\n",
       "           ...,\n",
       "           [-0.1199, -0.1453, -0.1282,  ..., -0.1063, -0.1203, -0.1199],\n",
       "           [-0.1199, -0.1064, -0.1143,  ..., -0.1098, -0.1183, -0.1199],\n",
       "           [-0.1199, -0.1199, -0.1199,  ..., -0.1199, -0.1199, -0.1199]],\n",
       " \n",
       "          [[ 0.2024,  0.2024,  0.2024,  ...,  0.2024,  0.2024,  0.2024],\n",
       "           [ 0.2024,  0.1937,  0.1838,  ...,  0.1789,  0.1832,  0.2024],\n",
       "           [ 0.2024,  0.2107,  0.2141,  ...,  0.1963,  0.1731,  0.2024],\n",
       "           ...,\n",
       "           [ 0.2024,  0.2060,  0.2067,  ...,  0.2018,  0.1993,  0.2024],\n",
       "           [ 0.2024,  0.1948,  0.1988,  ...,  0.2076,  0.2167,  0.2024],\n",
       "           [ 0.2024,  0.2024,  0.2024,  ...,  0.2024,  0.2024,  0.2024]]]],\n",
       "        grad_fn=<ThnnConv2DBackward>),\n",
       " 'w': tensor([[-0.0435, -0.0348,  0.0205,  0.0477, -0.0164, -0.0128, -0.0029, -0.0154,\n",
       "          -0.0100, -0.0031,  0.0290,  0.0176,  0.0698, -0.0622, -0.0146, -0.0372,\n",
       "           0.0086, -0.0069,  0.0163,  0.0223,  0.0318, -0.0095,  0.0328,  0.0591,\n",
       "          -0.0517, -0.0132, -0.0197,  0.0055,  0.0252, -0.0261,  0.0106,  0.0546,\n",
       "          -0.0427, -0.0029, -0.0188,  0.0314, -0.0490, -0.0063,  0.0261, -0.0222,\n",
       "          -0.0194,  0.0543, -0.0490, -0.0182, -0.0323, -0.0258,  0.0073,  0.0080,\n",
       "           0.0059,  0.0029, -0.0550,  0.0360, -0.0087,  0.0074, -0.0047, -0.0588,\n",
       "          -0.0420, -0.0453, -0.0208, -0.0535,  0.0277,  0.0630, -0.0201, -0.0127,\n",
       "           0.0132, -0.0181, -0.0305,  0.0270, -0.0144,  0.0811,  0.0309,  0.0176,\n",
       "           0.0454, -0.0316, -0.0220, -0.0260,  0.0357,  0.0492,  0.0190,  0.0504,\n",
       "          -0.0309,  0.0499,  0.0003,  0.0075,  0.0213, -0.0208,  0.0159, -0.0037,\n",
       "           0.0407,  0.0033,  0.0315, -0.0661,  0.0485, -0.0226,  0.0345,  0.0204,\n",
       "          -0.0506, -0.0212,  0.0150,  0.0145, -0.0418, -0.0360,  0.0750, -0.0100,\n",
       "          -0.0435,  0.0050, -0.0330, -0.0432,  0.0338, -0.0142,  0.0120, -0.0386,\n",
       "          -0.0284,  0.0070,  0.0161,  0.0366, -0.0173, -0.0448, -0.0389, -0.0006,\n",
       "          -0.0249,  0.0123,  0.0425,  0.0702, -0.0193,  0.0048,  0.0112,  0.0517,\n",
       "           0.0383,  0.0414, -0.0025, -0.0217, -0.0301, -0.0002,  0.0549,  0.0427,\n",
       "           0.0111, -0.0016, -0.0422, -0.0579,  0.0118, -0.0296,  0.0382, -0.0198,\n",
       "          -0.0493, -0.0530,  0.0262, -0.0001,  0.0135, -0.0299, -0.0665,  0.0385,\n",
       "           0.0247, -0.0470, -0.0054, -0.0012, -0.0281, -0.0262,  0.0236,  0.0183,\n",
       "           0.0139,  0.0201,  0.0033,  0.0402, -0.0172,  0.0336, -0.0558,  0.0248,\n",
       "           0.0305, -0.0458,  0.0324, -0.0140,  0.0229,  0.0336, -0.0037, -0.0076,\n",
       "          -0.0476, -0.0274,  0.0757,  0.0221,  0.0600,  0.0483,  0.0021,  0.0308,\n",
       "          -0.0299,  0.0413, -0.0030,  0.0206, -0.0267, -0.0111, -0.0373,  0.0758,\n",
       "          -0.0622,  0.0007,  0.0802, -0.0029, -0.0082,  0.0244, -0.0264, -0.0582,\n",
       "           0.0180,  0.0089,  0.0474,  0.0590,  0.0152,  0.0521,  0.0182,  0.0093,\n",
       "          -0.0289,  0.0231,  0.0319,  0.0145, -0.0424,  0.0636, -0.0228, -0.0416,\n",
       "          -0.0034, -0.0020,  0.0587, -0.0119, -0.0148, -0.0197,  0.0024,  0.0015,\n",
       "          -0.0159, -0.0329, -0.0710,  0.0304, -0.0187,  0.0107, -0.0459, -0.0266,\n",
       "          -0.0193,  0.0007, -0.0105,  0.0525,  0.0159, -0.0409,  0.0164,  0.0511,\n",
       "           0.0126,  0.0019, -0.0509, -0.0815,  0.0064,  0.0048, -0.0438,  0.0284,\n",
       "           0.0413,  0.0265,  0.0266, -0.0231,  0.0446,  0.0840,  0.0077, -0.0523,\n",
       "          -0.0485, -0.0373, -0.0468, -0.0207, -0.0105,  0.0283, -0.0499,  0.0033,\n",
       "          -0.0542, -0.0275,  0.0301,  0.0059, -0.0236,  0.0205,  0.0357,  0.0381,\n",
       "           0.0018,  0.0367,  0.0179, -0.0305, -0.0472,  0.0655, -0.0158,  0.0111,\n",
       "           0.0617, -0.0453, -0.0282,  0.0203, -0.0418, -0.0056, -0.0318, -0.0288,\n",
       "          -0.0318,  0.0019, -0.0285, -0.0561,  0.0536, -0.0455, -0.0401,  0.0440,\n",
       "          -0.0255, -0.0311, -0.0217,  0.0184,  0.0138, -0.0255,  0.0341,  0.0005,\n",
       "          -0.0688, -0.0242,  0.0335, -0.0170, -0.0211,  0.0415, -0.0299, -0.0064,\n",
       "          -0.0529, -0.0023,  0.0095,  0.0277,  0.0135,  0.0120, -0.0120,  0.0325,\n",
       "           0.0325,  0.0472,  0.0234,  0.0480,  0.0174, -0.0306,  0.0475, -0.0342,\n",
       "          -0.0461, -0.0544,  0.0383,  0.0398,  0.0209,  0.0235,  0.0227,  0.0173,\n",
       "           0.0045,  0.0669, -0.0454, -0.0116, -0.0335, -0.0484, -0.0006,  0.0233,\n",
       "          -0.0059, -0.0234,  0.0170,  0.0565, -0.0020,  0.0491,  0.0674,  0.0436,\n",
       "          -0.0135, -0.0357,  0.0314, -0.0327, -0.0207,  0.0699, -0.0079,  0.0045,\n",
       "          -0.0143,  0.0111,  0.0289, -0.0209,  0.0030, -0.0491, -0.0235, -0.0222,\n",
       "           0.0103,  0.0110,  0.0046,  0.0052, -0.0021, -0.0085, -0.0043,  0.0154,\n",
       "           0.0069,  0.0431, -0.0057,  0.0326, -0.0708,  0.0235, -0.0699,  0.0183,\n",
       "          -0.0267, -0.0091,  0.0180,  0.0048, -0.0197, -0.0120,  0.0171, -0.0293,\n",
       "           0.0044,  0.0028, -0.0132, -0.0179,  0.0453, -0.0532, -0.0596,  0.0489,\n",
       "           0.0305,  0.0583,  0.0550, -0.0291,  0.0070,  0.0125, -0.0302, -0.0447,\n",
       "          -0.0168, -0.0193,  0.0213, -0.0238, -0.0129,  0.0185,  0.0248, -0.0020,\n",
       "          -0.0157, -0.0055, -0.0105,  0.0202,  0.0113,  0.0085,  0.0192,  0.0418,\n",
       "           0.0132, -0.0102, -0.0093, -0.0660,  0.0613, -0.0479,  0.0485, -0.0110,\n",
       "           0.0226, -0.0288, -0.0562, -0.0670,  0.0034, -0.0301, -0.0071,  0.0369,\n",
       "          -0.0710,  0.0460,  0.0246, -0.0422, -0.0347, -0.0592, -0.0208,  0.0540,\n",
       "          -0.0327,  0.0634,  0.0116, -0.0379,  0.0244,  0.0026, -0.0549,  0.0582,\n",
       "          -0.0020,  0.0046,  0.0547, -0.0324, -0.0374, -0.0283,  0.0353, -0.0755,\n",
       "          -0.0171, -0.0024, -0.0615,  0.0272, -0.0451, -0.0237,  0.0098,  0.0636,\n",
       "          -0.0004,  0.0090, -0.0276, -0.0498, -0.0090,  0.0075, -0.0410, -0.0138,\n",
       "          -0.0121, -0.0710, -0.0389, -0.0287,  0.0524, -0.0057, -0.0128,  0.0279,\n",
       "          -0.0111, -0.0053,  0.0217, -0.0381,  0.0115, -0.0216, -0.0095,  0.0378,\n",
       "          -0.0356, -0.0238, -0.0428, -0.0427, -0.0205, -0.0472,  0.0374,  0.0035,\n",
       "           0.0157, -0.0145,  0.0311, -0.0299,  0.0295,  0.0218, -0.0181, -0.0213]],\n",
       "        grad_fn=<AddmmBackward>)}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(4, 512, 4, 4, requires_grad=True)\n",
    "z = torch.randn(1, 512, requires_grad=True)\n",
    "\n",
    "w = mn(z)\n",
    "\n",
    "\n",
    "tensor_dict = {\"x\": x, \"w\": w}\n",
    "\n",
    "sg(tensor_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(sg.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24102067"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc = 0\n",
    "for p in params:\n",
    "    s = list(p.shape)\n",
    "    c = 1\n",
    "    for n in s:\n",
    "        c *= n\n",
    "    pc += c\n",
    "\n",
    "pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
